{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D&D Dice Fraud Detection Software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Global Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please keep this area tidy. Import only what you need and big heavy stuff like sklearn / TensorFlow towards the end so that we don't run too quickly into Environment problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real data is generated by the formula as printed in the book: 4d6dl1 i.e. the sum of four random numbers in (1,6) of which the lowest is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_real=50000\n",
    "N_fake=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FourSixSidedDiceDropLowest():\n",
    "    randnums= np.random.randint(1,7,4)\n",
    "    return sum(np.delete(np.sort(randnums),0))\n",
    "\n",
    "RealSample=[FourSixSidedDiceDropLowest() for i in range(N_real)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate different types of faked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must use a great variety of formulae or algorithms in order to present faked data.\n",
    "Main suggestions:\n",
    "* More dice, more dropped: 5d6dl2 will make the average go up while keeping the upper bound the same\n",
    "* Smaller dice: 6d4dl2 has the same upper bound as 4d6dl1 but a higher average\n",
    "* Drop fewer dice: more extreme outliers can be generated by removing fewer dice. This pushes the upper bound beyond 18, which is obviously impossible, but the algorithm should see it to know. \n",
    "* Roll dice set multiple times and keep the \"best\": high-average, low-variance stat sets are infrequent but manifestly better. An example ''goodness indicator'' is the following formula: mu / (5 + sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbersizedrop(number,size,drop):\n",
    "    randnums= np.random.randint(1,size+1,number)\n",
    "    return sum(np.delete(np.sort(randnums),range(drop)))\n",
    "\n",
    "def keeprolling():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assemble data in Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pandas DataFrame object is most naturally suited for statistical analysis and regression. Note that we should keep the Real and Faked data separate until the end of preprocessing in order to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now would be a good time to save the raw data in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must perform a variety of operations on the data before it is analysed by the ML algorithm.\n",
    "* Feature scaling: remove mean and scale by variance to settle the data on an even footing\n",
    "* Randomising: Shuffling the data inside the DataFrames in order to remove correlations\n",
    "* Balancing: Ideally the ML algorithm has about as many data points in each category when trying to sort categorical data, which in this case means aiming for a 50% true/false split. It is easiest to perform this operation here, since we must come up with many different ways of generating fake data, it will be tedious to control how much of the fake data we generate.\n",
    "* Merge: both sets need to be in one DataFrame by the end of it, and we shuffle them once more\n",
    "* Train/Validate/Test split: Neural Networks have a natural tendency to completely overfit the data we give them. A part of the data is reserved for validation and testing. Fitting stops when Validation loss stops decreasing. \n",
    "* Batching: For increased speed, update coefficients based only on small batches of the training data each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validate/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now would also be a good time to saw the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and Validate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO NOT** I repeat ***DO NOT*** fiddle with the metaparameters to improve test data accuracy **WITHOUT** generating a new set of data samples. You WILL overfit your mum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
