{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D&D Dice Fraud Detection Software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Global Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please keep this area tidy. Import only what you need and big heavy stuff like sklearn / TensorFlow towards the end so that we don't run too quickly into Environment problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real data is generated by the formula as printed in the book: 4d6dl1 i.e. the sum of four random numbers in (1,6) of which the lowest is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_real=50000\n",
    "N_fake=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FourSixSidedDiceDropLowest():\n",
    "    randnums= np.random.randint(1,7,4)\n",
    "    return sum(np.delete(np.sort(randnums),0))\n",
    "\n",
    "realSample=np.array([[FourSixSidedDiceDropLowest() for i in range(6)] for i in range(N_real)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate different types of faked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must use a great variety of formulae or algorithms in order to present faked data.\n",
    "Main suggestions:\n",
    "* More dice, more dropped: 5d6dl2 will make the average go up while keeping the upper bound the same. Up to and including (x+1)d20dlx to make it very obvious\n",
    "* Smaller dice: 6d4dl2 has lower upper bound as 4d6dl1 but a higher average\n",
    "* Drop fewer dice: more extreme outliers can be generated by removing fewer dice. This pushes the upper bound beyond 18, which is obviously impossible, but the algorithm should see it to know. \n",
    "* Roll dice set multiple times and keep the \"best\": high-average, low-variance stat sets are infrequent but manifestly better. An example ''goodness indicator'' is the following formula: mu / (5 + sigma)\n",
    "* For good measure, ''handicap'' sets should also be given, so that the algorithm doesn't automatically think any statistical upper outlier is a cheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function implements any arbitrary XdYdlZ formula. All dice are assumed identical.\n",
    "def numbersizedrop(number,size,drop):\n",
    "    randnums= np.random.randint(1,size+1,number)\n",
    "    return sum(np.delete(np.sort(randnums),range(drop)))\n",
    "\n",
    "#This implements the \"get lucky\" algorithm whereby we keep rolling until\n",
    "#we find a good statistical outlier with high average and low variance\n",
    "def keeprolling(number,size,drop,targetcoeff):\n",
    "    coeff=0\n",
    "    i=0\n",
    "    while coeff < targetcoeff:\n",
    "        tstats= np.array([numbersizedrop(number,size,drop) for i in range(6)])\n",
    "        i+=1\n",
    "        if coeff < 2*np.mean(tstats) / (5+ np.std(tstats)):\n",
    "            coeff =2*np.mean(tstats) / (5+ np.std(tstats))\n",
    "            stats=tstats\n",
    "            #print('New candidate at step {2}: coeff. {0} with stats {1}'.format(coeff, stats, i))\n",
    "    #print('Final candidate at step {2}, coeff {0} with stats {1}'.format(coeff, stats,i))\n",
    "    return stats    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use a concatenation of ndarrays so that the code is somewhat scalable.\n",
    "#Adding a new generation method is then simply a case of pasting another block.\n",
    "#This also makes converting the data easier since we can use an enumerator.\n",
    "fakeSample=np.array([[[numbersizedrop(5,6,2) for i in range(6)] for i in range(N_fake)]])\n",
    "\n",
    "fakeSample=np.concatenate(\n",
    "    (\n",
    "        fakeSample,\n",
    "         np.array([[[numbersizedrop(6,4,2) for i in range(6)] for i in range(N_fake)]])\n",
    "    )\n",
    "    ,axis=0)\n",
    "\n",
    "fakeSample=np.concatenate(\n",
    "    (\n",
    "        fakeSample,\n",
    "         np.array([[[numbersizedrop(6,20,5) for i in range(6)] for i in range(N_fake)]])\n",
    "    )\n",
    "    ,axis=0)\n",
    "\n",
    "fakeSample=np.concatenate(\n",
    "    (\n",
    "        fakeSample,\n",
    "         np.array([[[numbersizedrop(3,20,2) for i in range(6)] for i in range(N_fake)]])\n",
    "    )\n",
    "    ,axis=0)\n",
    "\n",
    "fakeSample=np.concatenate(\n",
    "    (\n",
    "        fakeSample,\n",
    "         np.array([[[numbersizedrop(4,6,2) for i in range(6)] for i in range(N_fake)]])\n",
    "    )\n",
    "    ,axis=0)\n",
    "\n",
    "fakeSample=np.concatenate(\n",
    "    (\n",
    "        fakeSample,\n",
    "         np.array([[[numbersizedrop(1,20,0) for i in range(6)] for i in range(N_fake)]])\n",
    "    )\n",
    "    ,axis=0)\n",
    "\n",
    "fakeSample=np.concatenate(\n",
    "    (\n",
    "        fakeSample,\n",
    "         np.array([[[numbersizedrop(4,4,0) for i in range(6)] for i in range(N_fake)]])\n",
    "    )\n",
    "    ,axis=0)\n",
    "\n",
    "fakeSample=np.concatenate(\n",
    "    (\n",
    "        fakeSample,\n",
    "         np.array([[[numbersizedrop(2,8,0) for i in range(6)] for i in range(N_fake)]])\n",
    "    )\n",
    "    ,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(fakeSample)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assemble data in Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pandas DataFrame object is most naturally suited for statistical analysis and regression. Note that we should keep the Real and Faked data separate until the end of preprocessing in order to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.0000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>12.232000</td>\n",
       "      <td>12.251140</td>\n",
       "      <td>12.249360</td>\n",
       "      <td>12.256240</td>\n",
       "      <td>12.2455</td>\n",
       "      <td>12.248560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.844169</td>\n",
       "      <td>2.839104</td>\n",
       "      <td>2.835832</td>\n",
       "      <td>2.838926</td>\n",
       "      <td>2.8485</td>\n",
       "      <td>2.842059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1             2             3             4           5  \\\n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.0000   \n",
       "mean      12.232000     12.251140     12.249360     12.256240     12.2455   \n",
       "std        2.844169      2.839104      2.835832      2.838926      2.8485   \n",
       "min        3.000000      3.000000      3.000000      3.000000      3.0000   \n",
       "25%       10.000000     10.000000     10.000000     10.000000     10.0000   \n",
       "50%       12.000000     12.000000     12.000000     12.000000     12.0000   \n",
       "75%       14.000000     14.000000     14.000000     14.000000     14.0000   \n",
       "max       18.000000     18.000000     18.000000     18.000000     18.0000   \n",
       "\n",
       "                  6  \n",
       "count  50000.000000  \n",
       "mean      12.248560  \n",
       "std        2.842059  \n",
       "min        3.000000  \n",
       "25%       10.000000  \n",
       "50%       12.000000  \n",
       "75%       14.000000  \n",
       "max       18.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realDataRaw=pd.DataFrame(realSample, columns=['1','2','3','4','5','6'])\n",
    "realDataRaw.describe()\n",
    "#describe() explores the statistics of the DataFrame, very useful to compare the various generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeDataLists=[\n",
    "    pd.DataFrame(fakeSample[i], columns=['1','2','3','4','5','6']) for i in range(np.shape(fakeSample)[0])\n",
    "]\n",
    "#DataFrames are necessarily two-dimensional, so to start with we make one frame per method for fake data. \n",
    "#This allows us to explore the statistics of the fake data before fusing it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>13.445100</td>\n",
       "      <td>13.387900</td>\n",
       "      <td>13.405700</td>\n",
       "      <td>13.451700</td>\n",
       "      <td>13.397000</td>\n",
       "      <td>13.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.580243</td>\n",
       "      <td>2.593628</td>\n",
       "      <td>2.631083</td>\n",
       "      <td>2.613035</td>\n",
       "      <td>2.606505</td>\n",
       "      <td>2.606104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1             2             3             4             5  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      13.445100     13.387900     13.405700     13.451700     13.397000   \n",
       "std        2.580243      2.593628      2.631083      2.613035      2.606505   \n",
       "min        4.000000      3.000000      3.000000      4.000000      3.000000   \n",
       "25%       12.000000     12.000000     12.000000     12.000000     12.000000   \n",
       "50%       14.000000     14.000000     14.000000     14.000000     14.000000   \n",
       "75%       15.000000     15.000000     15.000000     15.000000     15.000000   \n",
       "max       18.000000     18.000000     18.000000     18.000000     18.000000   \n",
       "\n",
       "                  6  \n",
       "count  10000.000000  \n",
       "mean      13.404600  \n",
       "std        2.606104  \n",
       "min        3.000000  \n",
       "25%       12.000000  \n",
       "50%       14.000000  \n",
       "75%       15.000000  \n",
       "max       18.000000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeDataLists[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeDataRaw=pd.concat(\n",
    "    fakeDataLists \n",
    "    #,keys=[i+1 for i in range(np.shape(fakeSample)[0])\n",
    "    ,ignore_index=True\n",
    "          )\n",
    "#pd.concat takes an optional list of keys to create a hierarchy of column values.\n",
    "#Alternatively, flatten all the data together with the ignore_index optional argument, better suited for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now would be a good time to save the raw data in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "realDataRaw.to_csv('realDataRaw.csv')\n",
    "fakeDataRaw.to_csv('fakeDataRaw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must perform a variety of operations on the data before it is analysed by the ML algorithm.\n",
    "* Feature scaling: remove mean and scale by variance to settle the data on an even footing\n",
    "* Randomising: Shuffling the data inside the DataFrames in order to remove correlations\n",
    "* Balancing: Ideally the ML algorithm has about as many data points in each category when trying to sort categorical or binary data, which in this case means aiming for a 50% true/false split. It is easiest to perform this operation here, since we must come up with many different ways of generating fake data, it will be tedious to control how much of the fake data we generate.\n",
    "* Merge: both sets need to be in one DataFrame by the end of it, and we shuffle them once more\n",
    "* Train/Validate/Test split: Neural Networks have a natural tendency to completely overfit the data we give them. A part of the data is reserved for validation and testing. Fitting stops when Validation loss stops decreasing. \n",
    "* Batching: For increased speed, update coefficients based only on small batches of the training data each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validate/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now would also be a good time to saw the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and Validate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO NOT** I repeat ***DO NOT*** fiddle with the metaparameters to improve test data accuracy **WITHOUT** generating a new set of data samples. You WILL overfit your mum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
